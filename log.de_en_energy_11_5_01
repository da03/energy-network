train_energy.py --dataset iwslt/en-de --selfmode soft --encselfmode soft --temperature 0 --selftemperature 0 --encselftemperature 0 --encselfmode soft --save_to models/de-en/energy/11/default_5_01 --direction deen --train_from models/de-en/11/default_00_pretrain.e11.pt --yx 1 --xy 1
SRC Vocab Size: 39553, TRG Vocab Size: 39553
Building Model
Model(
  (encoder_fx): Encoder(
    (layers): ModuleList(
      (0): EncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linears): ModuleList(
            (0): Linear(in_features=278, out_features=278, bias=True)
            (1): Linear(in_features=278, out_features=278, bias=True)
            (2): Linear(in_features=278, out_features=278, bias=True)
            (3): Linear(in_features=278, out_features=278, bias=True)
          )
          (dropout): Dropout(p=0.1)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=278, out_features=507, bias=True)
          (w_2): Linear(in_features=507, out_features=278, bias=True)
          (dropout): Dropout(p=0.1)
        )
        (sublayer): ModuleList(
          (0): SublayerConnection(
            (norm): LayerNorm()
            (dropout): Dropout(p=0.1)
          )
          (1): SublayerConnection(
            (norm): LayerNorm()
            (dropout): Dropout(p=0.1)
          )
        )
      )
      (1): EncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linears): ModuleList(
            (0): Linear(in_features=278, out_features=278, bias=True)
            (1): Linear(in_features=278, out_features=278, bias=True)
            (2): Linear(in_features=278, out_features=278, bias=True)
            (3): Linear(in_features=278, out_features=278, bias=True)
          )
          (dropout): Dropout(p=0.1)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=278, out_features=507, bias=True)
          (w_2): Linear(in_features=507, out_features=278, bias=True)
          (dropout): Dropout(p=0.1)
        )
        (sublayer): ModuleList(
          (0): SublayerConnection(
            (norm): LayerNorm()
            (dropout): Dropout(p=0.1)
          )
          (1): SublayerConnection(
            (norm): LayerNorm()
            (dropout): Dropout(p=0.1)
          )
        )
      )
      (2): EncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linears): ModuleList(
            (0): Linear(in_features=278, out_features=278, bias=True)
            (1): Linear(in_features=278, out_features=278, bias=True)
            (2): Linear(in_features=278, out_features=278, bias=True)
            (3): Linear(in_features=278, out_features=278, bias=True)
          )
          (dropout): Dropout(p=0.1)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=278, out_features=507, bias=True)
          (w_2): Linear(in_features=507, out_features=278, bias=True)
          (dropout): Dropout(p=0.1)
        )
        (sublayer): ModuleList(
          (0): SublayerConnection(
            (norm): LayerNorm()
            (dropout): Dropout(p=0.1)
          )
          (1): SublayerConnection(
            (norm): LayerNorm()
            (dropout): Dropout(p=0.1)
          )
        )
      )
      (3): EncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linears): ModuleList(
            (0): Linear(in_features=278, out_features=278, bias=True)
            (1): Linear(in_features=278, out_features=278, bias=True)
            (2): Linear(in_features=278, out_features=278, bias=True)
            (3): Linear(in_features=278, out_features=278, bias=True)
          )
          (dropout): Dropout(p=0.1)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=278, out_features=507, bias=True)
          (w_2): Linear(in_features=507, out_features=278, bias=True)
          (dropout): Dropout(p=0.1)
        )
        (sublayer): ModuleList(
          (0): SublayerConnection(
            (norm): LayerNorm()
            (dropout): Dropout(p=0.1)
          )
          (1): SublayerConnection(
            (norm): LayerNorm()
            (dropout): Dropout(p=0.1)
          )
        )
      )
      (4): EncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linears): ModuleList(
            (0): Linear(in_features=278, out_features=278, bias=True)
            (1): Linear(in_features=278, out_features=278, bias=True)
            (2): Linear(in_features=278, out_features=278, bias=True)
            (3): Linear(in_features=278, out_features=278, bias=True)
          )
          (dropout): Dropout(p=0.1)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=278, out_features=507, bias=True)
          (w_2): Linear(in_features=507, out_features=278, bias=True)
          (dropout): Dropout(p=0.1)
        )
        (sublayer): ModuleList(
          (0): SublayerConnection(
            (norm): LayerNorm()
            (dropout): Dropout(p=0.1)
          )
          (1): SublayerConnection(
            (norm): LayerNorm()
            (dropout): Dropout(p=0.1)
          )
        )
      )
    )
    (norm): LayerNorm()
  )
  (encoder_fy): Encoder(
    (layers): ModuleList(
      (0): EncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linears): ModuleList(
            (0): Linear(in_features=278, out_features=278, bias=True)
            (1): Linear(in_features=278, out_features=278, bias=True)
            (2): Linear(in_features=278, out_features=278, bias=True)
            (3): Linear(in_features=278, out_features=278, bias=True)
          )
          (dropout): Dropout(p=0.1)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=278, out_features=507, bias=True)
          (w_2): Linear(in_features=507, out_features=278, bias=True)
          (dropout): Dropout(p=0.1)
        )
        (sublayer): ModuleList(
          (0): SublayerConnection(
            (norm): LayerNorm()
            (dropout): Dropout(p=0.1)
          )
          (1): SublayerConnection(
            (norm): LayerNorm()
            (dropout): Dropout(p=0.1)
          )
        )
      )
      (1): EncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linears): ModuleList(
            (0): Linear(in_features=278, out_features=278, bias=True)
            (1): Linear(in_features=278, out_features=278, bias=True)
            (2): Linear(in_features=278, out_features=278, bias=True)
            (3): Linear(in_features=278, out_features=278, bias=True)
          )
          (dropout): Dropout(p=0.1)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=278, out_features=507, bias=True)
          (w_2): Linear(in_features=507, out_features=278, bias=True)
          (dropout): Dropout(p=0.1)
        )
        (sublayer): ModuleList(
          (0): SublayerConnection(
            (norm): LayerNorm()
            (dropout): Dropout(p=0.1)
          )
          (1): SublayerConnection(
            (norm): LayerNorm()
            (dropout): Dropout(p=0.1)
          )
        )
      )
      (2): EncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linears): ModuleList(
            (0): Linear(in_features=278, out_features=278, bias=True)
            (1): Linear(in_features=278, out_features=278, bias=True)
            (2): Linear(in_features=278, out_features=278, bias=True)
            (3): Linear(in_features=278, out_features=278, bias=True)
          )
          (dropout): Dropout(p=0.1)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=278, out_features=507, bias=True)
          (w_2): Linear(in_features=507, out_features=278, bias=True)
          (dropout): Dropout(p=0.1)
        )
        (sublayer): ModuleList(
          (0): SublayerConnection(
            (norm): LayerNorm()
            (dropout): Dropout(p=0.1)
          )
          (1): SublayerConnection(
            (norm): LayerNorm()
            (dropout): Dropout(p=0.1)
          )
        )
      )
      (3): EncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linears): ModuleList(
            (0): Linear(in_features=278, out_features=278, bias=True)
            (1): Linear(in_features=278, out_features=278, bias=True)
            (2): Linear(in_features=278, out_features=278, bias=True)
            (3): Linear(in_features=278, out_features=278, bias=True)
          )
          (dropout): Dropout(p=0.1)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=278, out_features=507, bias=True)
          (w_2): Linear(in_features=507, out_features=278, bias=True)
          (dropout): Dropout(p=0.1)
        )
        (sublayer): ModuleList(
          (0): SublayerConnection(
            (norm): LayerNorm()
            (dropout): Dropout(p=0.1)
          )
          (1): SublayerConnection(
            (norm): LayerNorm()
            (dropout): Dropout(p=0.1)
          )
        )
      )
      (4): EncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linears): ModuleList(
            (0): Linear(in_features=278, out_features=278, bias=True)
            (1): Linear(in_features=278, out_features=278, bias=True)
            (2): Linear(in_features=278, out_features=278, bias=True)
            (3): Linear(in_features=278, out_features=278, bias=True)
          )
          (dropout): Dropout(p=0.1)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=278, out_features=507, bias=True)
          (w_2): Linear(in_features=507, out_features=278, bias=True)
          (dropout): Dropout(p=0.1)
        )
        (sublayer): ModuleList(
          (0): SublayerConnection(
            (norm): LayerNorm()
            (dropout): Dropout(p=0.1)
          )
          (1): SublayerConnection(
            (norm): LayerNorm()
            (dropout): Dropout(p=0.1)
          )
        )
      )
    )
    (norm): LayerNorm()
  )
  (decoder_gx): Decoder(
    (layers): ModuleList(
      (0): DecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linears): ModuleList(
            (0): Linear(in_features=278, out_features=278, bias=True)
            (1): Linear(in_features=278, out_features=278, bias=True)
            (2): Linear(in_features=278, out_features=278, bias=True)
            (3): Linear(in_features=278, out_features=278, bias=True)
          )
          (dropout): Dropout(p=0.1)
        )
        (src_attn): MultiHeadedAttention(
          (linears): ModuleList(
            (0): Linear(in_features=278, out_features=278, bias=True)
            (1): Linear(in_features=278, out_features=278, bias=True)
            (2): Linear(in_features=278, out_features=278, bias=True)
            (3): Linear(in_features=278, out_features=278, bias=True)
          )
          (dropout): Dropout(p=0.1)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=278, out_features=507, bias=True)
          (w_2): Linear(in_features=507, out_features=278, bias=True)
          (dropout): Dropout(p=0.1)
        )
        (sublayer): ModuleList(
          (0): SublayerConnection(
            (norm): LayerNorm()
            (dropout): Dropout(p=0.1)
          )
          (1): SublayerConnection(
            (norm): LayerNorm()
            (dropout): Dropout(p=0.1)
          )
          (2): SublayerConnection(
            (norm): LayerNorm()
            (dropout): Dropout(p=0.1)
          )
        )
      )
      (1): DecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linears): ModuleList(
            (0): Linear(in_features=278, out_features=278, bias=True)
            (1): Linear(in_features=278, out_features=278, bias=True)
            (2): Linear(in_features=278, out_features=278, bias=True)
            (3): Linear(in_features=278, out_features=278, bias=True)
          )
          (dropout): Dropout(p=0.1)
        )
        (src_attn): MultiHeadedAttention(
          (linears): ModuleList(
            (0): Linear(in_features=278, out_features=278, bias=True)
            (1): Linear(in_features=278, out_features=278, bias=True)
            (2): Linear(in_features=278, out_features=278, bias=True)
            (3): Linear(in_features=278, out_features=278, bias=True)
          )
          (dropout): Dropout(p=0.1)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=278, out_features=507, bias=True)
          (w_2): Linear(in_features=507, out_features=278, bias=True)
          (dropout): Dropout(p=0.1)
        )
        (sublayer): ModuleList(
          (0): SublayerConnection(
            (norm): LayerNorm()
            (dropout): Dropout(p=0.1)
          )
          (1): SublayerConnection(
            (norm): LayerNorm()
            (dropout): Dropout(p=0.1)
          )
          (2): SublayerConnection(
            (norm): LayerNorm()
            (dropout): Dropout(p=0.1)
          )
        )
      )
      (2): DecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linears): ModuleList(
            (0): Linear(in_features=278, out_features=278, bias=True)
            (1): Linear(in_features=278, out_features=278, bias=True)
            (2): Linear(in_features=278, out_features=278, bias=True)
            (3): Linear(in_features=278, out_features=278, bias=True)
          )
          (dropout): Dropout(p=0.1)
        )
        (src_attn): MultiHeadedAttention(
          (linears): ModuleList(
            (0): Linear(in_features=278, out_features=278, bias=True)
            (1): Linear(in_features=278, out_features=278, bias=True)
            (2): Linear(in_features=278, out_features=278, bias=True)
            (3): Linear(in_features=278, out_features=278, bias=True)
          )
          (dropout): Dropout(p=0.1)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=278, out_features=507, bias=True)
          (w_2): Linear(in_features=507, out_features=278, bias=True)
          (dropout): Dropout(p=0.1)
        )
        (sublayer): ModuleList(
          (0): SublayerConnection(
            (norm): LayerNorm()
            (dropout): Dropout(p=0.1)
          )
          (1): SublayerConnection(
            (norm): LayerNorm()
            (dropout): Dropout(p=0.1)
          )
          (2): SublayerConnection(
            (norm): LayerNorm()
            (dropout): Dropout(p=0.1)
          )
        )
      )
      (3): DecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linears): ModuleList(
            (0): Linear(in_features=278, out_features=278, bias=True)
            (1): Linear(in_features=278, out_features=278, bias=True)
            (2): Linear(in_features=278, out_features=278, bias=True)
            (3): Linear(in_features=278, out_features=278, bias=True)
          )
          (dropout): Dropout(p=0.1)
        )
        (src_attn): MultiHeadedAttention(
          (linears): ModuleList(
            (0): Linear(in_features=278, out_features=278, bias=True)
            (1): Linear(in_features=278, out_features=278, bias=True)
            (2): Linear(in_features=278, out_features=278, bias=True)
            (3): Linear(in_features=278, out_features=278, bias=True)
          )
          (dropout): Dropout(p=0.1)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=278, out_features=507, bias=True)
          (w_2): Linear(in_features=507, out_features=278, bias=True)
          (dropout): Dropout(p=0.1)
        )
        (sublayer): ModuleList(
          (0): SublayerConnection(
            (norm): LayerNorm()
            (dropout): Dropout(p=0.1)
          )
          (1): SublayerConnection(
            (norm): LayerNorm()
            (dropout): Dropout(p=0.1)
          )
          (2): SublayerConnection(
            (norm): LayerNorm()
            (dropout): Dropout(p=0.1)
          )
        )
      )
      (4): DecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linears): ModuleList(
            (0): Linear(in_features=278, out_features=278, bias=True)
            (1): Linear(in_features=278, out_features=278, bias=True)
            (2): Linear(in_features=278, out_features=278, bias=True)
            (3): Linear(in_features=278, out_features=278, bias=True)
          )
          (dropout): Dropout(p=0.1)
        )
        (src_attn): MultiHeadedAttention(
          (linears): ModuleList(
            (0): Linear(in_features=278, out_features=278, bias=True)
            (1): Linear(in_features=278, out_features=278, bias=True)
            (2): Linear(in_features=278, out_features=278, bias=True)
            (3): Linear(in_features=278, out_features=278, bias=True)
          )
          (dropout): Dropout(p=0.1)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=278, out_features=507, bias=True)
          (w_2): Linear(in_features=507, out_features=278, bias=True)
          (dropout): Dropout(p=0.1)
        )
        (sublayer): ModuleList(
          (0): SublayerConnection(
            (norm): LayerNorm()
            (dropout): Dropout(p=0.1)
          )
          (1): SublayerConnection(
            (norm): LayerNorm()
            (dropout): Dropout(p=0.1)
          )
          (2): SublayerConnection(
            (norm): LayerNorm()
            (dropout): Dropout(p=0.1)
          )
        )
      )
    )
    (norm): LayerNorm()
  )
  (decoder_gy): Decoder(
    (layers): ModuleList(
      (0): DecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linears): ModuleList(
            (0): Linear(in_features=278, out_features=278, bias=True)
            (1): Linear(in_features=278, out_features=278, bias=True)
            (2): Linear(in_features=278, out_features=278, bias=True)
            (3): Linear(in_features=278, out_features=278, bias=True)
          )
          (dropout): Dropout(p=0.1)
        )
        (src_attn): MultiHeadedAttention(
          (linears): ModuleList(
            (0): Linear(in_features=278, out_features=278, bias=True)
            (1): Linear(in_features=278, out_features=278, bias=True)
            (2): Linear(in_features=278, out_features=278, bias=True)
            (3): Linear(in_features=278, out_features=278, bias=True)
          )
          (dropout): Dropout(p=0.1)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=278, out_features=507, bias=True)
          (w_2): Linear(in_features=507, out_features=278, bias=True)
          (dropout): Dropout(p=0.1)
        )
        (sublayer): ModuleList(
          (0): SublayerConnection(
            (norm): LayerNorm()
            (dropout): Dropout(p=0.1)
          )
          (1): SublayerConnection(
            (norm): LayerNorm()
            (dropout): Dropout(p=0.1)
          )
          (2): SublayerConnection(
            (norm): LayerNorm()
            (dropout): Dropout(p=0.1)
          )
        )
      )
      (1): DecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linears): ModuleList(
            (0): Linear(in_features=278, out_features=278, bias=True)
            (1): Linear(in_features=278, out_features=278, bias=True)
            (2): Linear(in_features=278, out_features=278, bias=True)
            (3): Linear(in_features=278, out_features=278, bias=True)
          )
          (dropout): Dropout(p=0.1)
        )
        (src_attn): MultiHeadedAttention(
          (linears): ModuleList(
            (0): Linear(in_features=278, out_features=278, bias=True)
            (1): Linear(in_features=278, out_features=278, bias=True)
            (2): Linear(in_features=278, out_features=278, bias=True)
            (3): Linear(in_features=278, out_features=278, bias=True)
          )
          (dropout): Dropout(p=0.1)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=278, out_features=507, bias=True)
          (w_2): Linear(in_features=507, out_features=278, bias=True)
          (dropout): Dropout(p=0.1)
        )
        (sublayer): ModuleList(
          (0): SublayerConnection(
            (norm): LayerNorm()
            (dropout): Dropout(p=0.1)
          )
          (1): SublayerConnection(
            (norm): LayerNorm()
            (dropout): Dropout(p=0.1)
          )
          (2): SublayerConnection(
            (norm): LayerNorm()
            (dropout): Dropout(p=0.1)
          )
        )
      )
      (2): DecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linears): ModuleList(
            (0): Linear(in_features=278, out_features=278, bias=True)
            (1): Linear(in_features=278, out_features=278, bias=True)
            (2): Linear(in_features=278, out_features=278, bias=True)
            (3): Linear(in_features=278, out_features=278, bias=True)
          )
          (dropout): Dropout(p=0.1)
        )
        (src_attn): MultiHeadedAttention(
          (linears): ModuleList(
            (0): Linear(in_features=278, out_features=278, bias=True)
            (1): Linear(in_features=278, out_features=278, bias=True)
            (2): Linear(in_features=278, out_features=278, bias=True)
            (3): Linear(in_features=278, out_features=278, bias=True)
          )
          (dropout): Dropout(p=0.1)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=278, out_features=507, bias=True)
          (w_2): Linear(in_features=507, out_features=278, bias=True)
          (dropout): Dropout(p=0.1)
        )
        (sublayer): ModuleList(
          (0): SublayerConnection(
            (norm): LayerNorm()
            (dropout): Dropout(p=0.1)
          )
          (1): SublayerConnection(
            (norm): LayerNorm()
            (dropout): Dropout(p=0.1)
          )
          (2): SublayerConnection(
            (norm): LayerNorm()
            (dropout): Dropout(p=0.1)
          )
        )
      )
      (3): DecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linears): ModuleList(
            (0): Linear(in_features=278, out_features=278, bias=True)
            (1): Linear(in_features=278, out_features=278, bias=True)
            (2): Linear(in_features=278, out_features=278, bias=True)
            (3): Linear(in_features=278, out_features=278, bias=True)
          )
          (dropout): Dropout(p=0.1)
        )
        (src_attn): MultiHeadedAttention(
          (linears): ModuleList(
            (0): Linear(in_features=278, out_features=278, bias=True)
            (1): Linear(in_features=278, out_features=278, bias=True)
            (2): Linear(in_features=278, out_features=278, bias=True)
            (3): Linear(in_features=278, out_features=278, bias=True)
          )
          (dropout): Dropout(p=0.1)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=278, out_features=507, bias=True)
          (w_2): Linear(in_features=507, out_features=278, bias=True)
          (dropout): Dropout(p=0.1)
        )
        (sublayer): ModuleList(
          (0): SublayerConnection(
            (norm): LayerNorm()
            (dropout): Dropout(p=0.1)
          )
          (1): SublayerConnection(
            (norm): LayerNorm()
            (dropout): Dropout(p=0.1)
          )
          (2): SublayerConnection(
            (norm): LayerNorm()
            (dropout): Dropout(p=0.1)
          )
        )
      )
      (4): DecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linears): ModuleList(
            (0): Linear(in_features=278, out_features=278, bias=True)
            (1): Linear(in_features=278, out_features=278, bias=True)
            (2): Linear(in_features=278, out_features=278, bias=True)
            (3): Linear(in_features=278, out_features=278, bias=True)
          )
          (dropout): Dropout(p=0.1)
        )
        (src_attn): MultiHeadedAttention(
          (linears): ModuleList(
            (0): Linear(in_features=278, out_features=278, bias=True)
            (1): Linear(in_features=278, out_features=278, bias=True)
            (2): Linear(in_features=278, out_features=278, bias=True)
            (3): Linear(in_features=278, out_features=278, bias=True)
          )
          (dropout): Dropout(p=0.1)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=278, out_features=507, bias=True)
          (w_2): Linear(in_features=507, out_features=278, bias=True)
          (dropout): Dropout(p=0.1)
        )
        (sublayer): ModuleList(
          (0): SublayerConnection(
            (norm): LayerNorm()
            (dropout): Dropout(p=0.1)
          )
          (1): SublayerConnection(
            (norm): LayerNorm()
            (dropout): Dropout(p=0.1)
          )
          (2): SublayerConnection(
            (norm): LayerNorm()
            (dropout): Dropout(p=0.1)
          )
        )
      )
    )
    (norm): LayerNorm()
  )
  (init_x): Sequential(
    (0): Linear(in_features=278, out_features=507, bias=True)
    (1): LeakyReLU(negative_slope=0.01)
    (2): Linear(in_features=507, out_features=278, bias=True)
  )
  (init_y): Sequential(
    (0): Linear(in_features=278, out_features=507, bias=True)
    (1): LeakyReLU(negative_slope=0.01)
    (2): Linear(in_features=507, out_features=278, bias=True)
  )
  (src_embed): Sequential(
    (0): Embeddings(
      (lut): Embedding(39553, 278)
    )
    (1): PositionalEncoding(
      (dropout): Dropout(p=0.1)
    )
  )
  (trg_embed): Sequential(
    (0): Embeddings(
      (lut): Embedding(39553, 278)
    )
    (1): PositionalEncoding(
      (dropout): Dropout(p=0.1)
    )
  )
  (generator_gx): Generator(
    (proj): Linear(in_features=278, out_features=39553, bias=True)
  )
  (generator_gy): Generator(
    (proj): Linear(in_features=278, out_features=39553, bias=True)
  )
)
Loading Model from models/de-en/11/default_00_pretrain.e11.pt
Namespace(accum_grad=1, anneal_kl=1, anneal_steps=250000, anneal_temperature=0, batch_size=3200, dataset='iwslt/en-de', dependent_posterior=1, direction='deen', dropout=0.1, encselfmode='soft', encselftemperature=0.0, epochs=50, eta=0.1, fix_model_steps=-1, heads=2, learning_rate=0.001, lr_begin=0.00015, lr_end=1e-05, max_src_len=150, max_trg_len=150, mode='soft', residual_var=0, save_to='models/de-en/energy/11/default_5_01', selfdependent_posterior=1, selfmode='soft', selftemperature=0.0, share_decoder_embeddings=1, share_word_embeddings=0, sharelstm=0, temperature=0.0, train_from='models/de-en/11/default_00_pretrain.e11.pt', unroll=5, xx=1, xy=1, yx=1, yy=1)

Epoch: 0
Training
Epoch Step: 49 lr: 0.000000, PPL yx: 15.218792, Acc yx: 0.463567, PPL xy: 21027.511112, Acc xy: 0.101171, PPL xx: 1.000000, Acc xx: 0.000000, PPL yy: 1.000000, Acc yy: 0.000000 , Tokens per Sec: 0.000000
Epoch Step: 99 lr: 0.000000, PPL yx: 15.078418, Acc yx: 0.462811, PPL xy: 20327.622312, Acc xy: 0.105644, PPL xx: 1.000000, Acc xx: 0.000000, PPL yy: 1.000000, Acc yy: 0.000000 , Tokens per Sec: 0.000000
Epoch Step: 149 lr: 0.000000, PPL yx: 15.995202, Acc yx: 0.456579, PPL xy: 21847.164241, Acc xy: 0.101552, PPL xx: 1.000000, Acc xx: 0.000000, PPL yy: 1.000000, Acc yy: 0.000000 , Tokens per Sec: 0.000000
Epoch Step: 199 lr: 0.000000, PPL yx: 17.156528, Acc yx: 0.441722, PPL xy: 25932.364229, Acc xy: 0.097937, PPL xx: 1.000000, Acc xx: 0.000000, PPL yy: 1.000000, Acc yy: 0.000000 , Tokens per Sec: 0.000000
Epoch Step: 249 lr: 0.000000, PPL yx: 15.672126, Acc yx: 0.457882, PPL xy: 21701.307990, Acc xy: 0.100428, PPL xx: 1.000000, Acc xx: 0.000000, PPL yy: 1.000000, Acc yy: 0.000000 , Tokens per Sec: 0.000000
Epoch Step: 299 lr: 0.000000, PPL yx: 16.746303, Acc yx: 0.444618, PPL xy: 23309.107800, Acc xy: 0.102096, PPL xx: 1.000000, Acc xx: 0.000000, PPL yy: 1.000000, Acc yy: 0.000000 , Tokens per Sec: 0.000000
Epoch Step: 349 lr: 0.000000, PPL yx: 14.033331, Acc yx: 0.476367, PPL xy: 17836.956332, Acc xy: 0.104565, PPL xx: 1.000000, Acc xx: 0.000000, PPL yy: 1.000000, Acc yy: 0.000000 , Tokens per Sec: 0.000000
Epoch Step: 399 lr: 0.000000, PPL yx: 15.803868, Acc yx: 0.457237, PPL xy: 21009.059935, Acc xy: 0.102093, PPL xx: 1.000000, Acc xx: 0.000000, PPL yy: 1.000000, Acc yy: 0.000000 , Tokens per Sec: 0.000000
Epoch Step: 449 lr: 0.000000, PPL yx: 15.855061, Acc yx: 0.453890, PPL xy: 20893.607983, Acc xy: 0.107100, PPL xx: 1.000000, Acc xx: 0.000000, PPL yy: 1.000000, Acc yy: 0.000000 , Tokens per Sec: 0.000000
Epoch Step: 499 lr: 0.000000, PPL yx: 14.129949, Acc yx: 0.479841, PPL xy: 19006.048585, Acc xy: 0.103689, PPL xx: 1.000000, Acc xx: 0.000000, PPL yy: 1.000000, Acc yy: 0.000000 , Tokens per Sec: 0.000000
Epoch Step: 549 lr: 0.000000, PPL yx: 15.539891, Acc yx: 0.458757, PPL xy: 20743.515435, Acc xy: 0.102853, PPL xx: 1.000000, Acc xx: 0.000000, PPL yy: 1.000000, Acc yy: 0.000000 , Tokens per Sec: 0.000000
Epoch Step: 599 lr: 0.000000, PPL yx: 17.478300, Acc yx: 0.441488, PPL xy: 27111.297017, Acc xy: 0.092049, PPL xx: 1.000000, Acc xx: 0.000000, PPL yy: 1.000000, Acc yy: 0.000000 , Tokens per Sec: 0.000000
Epoch Step: 649 lr: 0.000000, PPL yx: 17.028780, Acc yx: 0.443310, PPL xy: 23443.529170, Acc xy: 0.098350, PPL xx: 1.000000, Acc xx: 0.000000, PPL yy: 1.000000, Acc yy: 0.000000 , Tokens per Sec: 0.000000
Epoch Step: 699 lr: 0.000000, PPL yx: 14.299715, Acc yx: 0.475297, PPL xy: 20178.119666, Acc xy: 0.097984, PPL xx: 1.000000, Acc xx: 0.000000, PPL yy: 1.000000, Acc yy: 0.000000 , Tokens per Sec: 0.000000
Epoch Step: 749 lr: 0.000000, PPL yx: 14.869134, Acc yx: 0.470889, PPL xy: 19517.344346, Acc xy: 0.102501, PPL xx: 1.000000, Acc xx: 0.000000, PPL yy: 1.000000, Acc yy: 0.000000 , Tokens per Sec: 0.000000
Epoch Step: 799 lr: 0.000000, PPL yx: 14.632001, Acc yx: 0.469040, PPL xy: 19104.474635, Acc xy: 0.103839, PPL xx: 1.000000, Acc xx: 0.000000, PPL yy: 1.000000, Acc yy: 0.000000 , Tokens per Sec: 0.000000
Epoch Step: 849 lr: 0.000000, PPL yx: 16.543013, Acc yx: 0.447715, PPL xy: 21481.770068, Acc xy: 0.101104, PPL xx: 1.000000, Acc xx: 0.000000, PPL yy: 1.000000, Acc yy: 0.000000 , Tokens per Sec: 0.000000
Epoch Step: 899 lr: 0.000000, PPL yx: 14.093092, Acc yx: 0.478729, PPL xy: 18199.400130, Acc xy: 0.104686, PPL xx: 1.000000, Acc xx: 0.000000, PPL yy: 1.000000, Acc yy: 0.000000 , Tokens per Sec: 0.000000
Epoch Step: 949 lr: 0.000000, PPL yx: 14.334291, Acc yx: 0.472124, PPL xy: 18038.730933, Acc xy: 0.105675, PPL xx: 1.000000, Acc xx: 0.000000, PPL yy: 1.000000, Acc yy: 0.000000 , Tokens per Sec: 0.000000
Epoch Step: 999 lr: 0.000000, PPL yx: 15.989878, Acc yx: 0.455507, PPL xy: 22596.101642, Acc xy: 0.095290, PPL xx: 1.000000, Acc xx: 0.000000, PPL yy: 1.000000, Acc yy: 0.000000 , Tokens per Sec: 0.000000
Epoch Step: 1049 lr: 0.000000, PPL yx: 14.504470, Acc yx: 0.472295, PPL xy: 21518.885968, Acc xy: 0.097291, PPL xx: 1.000000, Acc xx: 0.000000, PPL yy: 1.000000, Acc yy: 0.000000 , Tokens per Sec: 0.000000
Epoch Step: 1099 lr: 0.000000, PPL yx: 15.802820, Acc yx: 0.454534, PPL xy: 20626.411426, Acc xy: 0.103699, PPL xx: 1.000000, Acc xx: 0.000000, PPL yy: 1.000000, Acc yy: 0.000000 , Tokens per Sec: 0.000000
Epoch Step: 1149 lr: 0.000000, PPL yx: 14.257157, Acc yx: 0.474645, PPL xy: 18019.458304, Acc xy: 0.107067, PPL xx: 1.000000, Acc xx: 0.000000, PPL yy: 1.000000, Acc yy: 0.000000 , Tokens per Sec: 0.000000
Epoch Step: 1199 lr: 0.000000, PPL yx: 14.166201, Acc yx: 0.475165, PPL xy: 17126.057676, Acc xy: 0.107514, PPL xx: 1.000000, Acc xx: 0.000000, PPL yy: 1.000000, Acc yy: 0.000000 , Tokens per Sec: 0.000000
Epoch Step: 1249 lr: 0.000000, PPL yx: 15.412494, Acc yx: 0.463819, PPL xy: 19730.801353, Acc xy: 0.101311, PPL xx: 1.000000, Acc xx: 0.000000, PPL yy: 1.000000, Acc yy: 0.000000 , Tokens per Sec: 0.000000
Epoch Step: 1299 lr: 0.000000, PPL yx: 15.398698, Acc yx: 0.460123, PPL xy: 20623.132998, Acc xy: 0.100894, PPL xx: 1.000000, Acc xx: 0.000000, PPL yy: 1.000000, Acc yy: 0.000000 , Tokens per Sec: 0.000000
Epoch Step: 1349 lr: 0.000000, PPL yx: 14.025485, Acc yx: 0.479160, PPL xy: 18975.969095, Acc xy: 0.100888, PPL xx: 1.000000, Acc xx: 0.000000, PPL yy: 1.000000, Acc yy: 0.000000 , Tokens per Sec: 0.000000
Epoch Step: 1399 lr: 0.000000, PPL yx: 16.021356, Acc yx: 0.458578, PPL xy: 22377.654112, Acc xy: 0.098489, PPL xx: 1.000000, Acc xx: 0.000000, PPL yy: 1.000000, Acc yy: 0.000000 , Tokens per Sec: 0.000000
Epoch Step: 1449 lr: 0.000000, PPL yx: 15.359319, Acc yx: 0.460528, PPL xy: 20369.015313, Acc xy: 0.104439, PPL xx: 1.000000, Acc xx: 0.000000, PPL yy: 1.000000, Acc yy: 0.000000 , Tokens per Sec: 0.000000
Validation
Val Result (0): PPL yx: 26.048897, Acc yx: 0.414993. PPL xy: 1.000000, Acc xy: 0.000000. PPL xx: 1.000000, Acc xx: 0.000000. PPL yy: 1.000000, Acc yy: 0.000000
Val Result (1): PPL yx: 26.085629, Acc yx: 0.414950. PPL xy: 1.000000, Acc xy: 0.000000. PPL xx: 1.000000, Acc xx: 0.000000. PPL yy: 1.000000, Acc yy: 0.000000
Val Result (2): PPL yx: 26.152984, Acc yx: 0.413781. PPL xy: 1.000000, Acc xy: 0.000000. PPL xx: 1.000000, Acc xx: 0.000000. PPL yy: 1.000000, Acc yy: 0.000000
Val Result (3): PPL yx: 26.254832, Acc yx: 0.412699. PPL xy: 1.000000, Acc xy: 0.000000. PPL xx: 1.000000, Acc xx: 0.000000. PPL yy: 1.000000, Acc yy: 0.000000
Val Result (4): PPL yx: 26.392618, Acc yx: 0.412439. PPL xy: 1.000000, Acc xy: 0.000000. PPL xx: 1.000000, Acc xx: 0.000000. PPL yy: 1.000000, Acc yy: 0.000000
Val Result (5): PPL yx: 26.035173, Acc yx: 0.414604. PPL xy: 1.000000, Acc xy: 0.000000. PPL xx: 1.000000, Acc xx: 0.000000. PPL yy: 1.000000, Acc yy: 0.000000

Epoch: 1
Training
Epoch Step: 49 lr: 0.000000, PPL yx: 15.705697, Acc yx: 0.458158, PPL xy: 21063.931357, Acc xy: 0.102607, PPL xx: 1.000000, Acc xx: 0.000000, PPL yy: 1.000000, Acc yy: 0.000000 , Tokens per Sec: 0.000000
Epoch Step: 99 lr: 0.000000, PPL yx: 14.525875, Acc yx: 0.471586, PPL xy: 19352.741715, Acc xy: 0.100995, PPL xx: 1.000000, Acc xx: 0.000000, PPL yy: 1.000000, Acc yy: 0.000000 , Tokens per Sec: 0.000000
Epoch Step: 149 lr: 0.000000, PPL yx: 13.486899, Acc yx: 0.486170, PPL xy: 18621.671350, Acc xy: 0.098144, PPL xx: 1.000000, Acc xx: 0.000000, PPL yy: 1.000000, Acc yy: 0.000000 , Tokens per Sec: 0.000000
Epoch Step: 199 lr: 0.000000, PPL yx: 15.631024, Acc yx: 0.460375, PPL xy: 20228.771719, Acc xy: 0.100868, PPL xx: 1.000000, Acc xx: 0.000000, PPL yy: 1.000000, Acc yy: 0.000000 , Tokens per Sec: 0.000000
